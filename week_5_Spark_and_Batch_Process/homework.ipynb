{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a9a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b6bef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/sammygis/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/03/06 18:51:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark =  SparkSession.builder \\\n",
    "    .master('local[*]')\\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60452e6",
   "metadata": {},
   "source": [
    "Home Work Solution 1\n",
    "\n",
    "Install Spark and PySpark\n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "- What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aba7b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754a97e",
   "metadata": {},
   "source": [
    "Home work solution 2\n",
    "\n",
    "Data: FHV October 2019\n",
    "\n",
    "- Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "- Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "- What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33fa76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756dc5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62958 fhv_tripdata_2019-10.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e027b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = spark.read\\\n",
    "    .option('header','true') \\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f965bcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropOff_datetime,StringType,true),StructField(PUlocationID,StringType,true),StructField(DOlocationID,StringType,true),StructField(SR_Flag,StringType,true),StructField(Affiliated_base_number,StringType,true)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dd65dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01670abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "import pandas as pd\n",
    "df_pandas = pd.read_csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35bfc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num       object\n",
       "pickup_datetime            object\n",
       "dropOff_datetime           object\n",
       "PUlocationID              float64\n",
       "DOlocationID              float64\n",
       "SR_Flag                   float64\n",
       "Affiliated_base_number     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2761e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas['Affiliated_base_number'] = df_pandas['Affiliated_base_number'].fillna(\"NAN Values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b602c5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropOff_datetime,StringType,true),StructField(PUlocationID,DoubleType,true),StructField(DOlocationID,DoubleType,true),StructField(SR_Flag,DoubleType,true),StructField(Affiliated_base_number,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_pandas).schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45c285",
   "metadata": {},
   "source": [
    "Define the new shema and read the data again by pasisng in the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "903686aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num',types.StringType(),True),\n",
    "    types.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "    types.StructField('dropOff_datetime',types.TimestampType(),True),\n",
    "    types.StructField('PUlocationID',types.IntegerType(),True),\n",
    "    types.StructField('DOlocationID',types.IntegerType(),True),\n",
    "    types.StructField('SR_Flag',types.StringType(),True),\n",
    "    types.StructField('Affiliated_base_number',types.StringType(),True)\n",
    "])\n",
    "\n",
    "df_fhv = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema)\\\n",
    "    .csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af604380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,TimestampType,true),StructField(dropOff_datetime,TimestampType,true),StructField(PUlocationID,IntegerType,true),StructField(DOlocationID,IntegerType,true),StructField(SR_Flag,StringType,true),StructField(Affiliated_base_number,StringType,true)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "849e1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv = df_fhv.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d24dc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.write.parquet('fhvhv/2019/01', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ef040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 38M\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.3M Mar  6 18:53 part-00005-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.4M Mar  6 18:53 part-00004-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.4M Mar  6 18:53 part-00003-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.4M Mar  6 18:53 part-00002-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.4M Mar  6 18:53 part-00001-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis 6.4M Mar  6 18:53 part-00000-179f9f23-2d95-4cc1-a9f9-5ee3c3f2f13e-c000.snappy.parquet\r\n",
      "-rw-r--r-- 1 sammygis sammygis    0 Mar  6 18:53 _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lhr fhvhv/2019/01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cb3cb1",
   "metadata": {},
   "source": [
    "Homework Solution 3:\n",
    "\n",
    "Count records\n",
    "- How many taxi trips were there on the 15th of October?\n",
    "- Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13a688d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   62610|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fhv.registerTempTable(\"fhvhv\")\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM fhvhv\n",
    "    WHERE to_date(pickup_datetime) = '2019-10-15'\n",
    "    \"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94e010",
   "metadata": {},
   "source": [
    "Homework Solution 4\n",
    "\n",
    "Longest trip for each day\n",
    "- What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9be6ab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:==================================================>    (182 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+------------------------------+\n",
      "|pickup_date|    pickup_datetime|   dropOff_datetime|longest_trip_duration_in_hours|\n",
      "+-----------+-------------------+-------------------+------------------------------+\n",
      "| 2019-10-02|2019-10-02 00:37:00|2019-10-02 23:49:00|                     24.433334|\n",
      "| 2019-10-17|2019-10-17 00:47:00|2019-10-17 23:36:00|                     24.383333|\n",
      "| 2019-10-15|2019-10-15 00:49:00|2019-10-15 23:17:00|                     24.100000|\n",
      "| 2019-10-16|2019-10-16 00:45:00|2019-10-16 23:13:00|                     23.966667|\n",
      "| 2019-10-16|2019-10-16 00:21:00|2019-10-16 22:57:00|                     23.300000|\n",
      "| 2019-10-25|2019-10-25 00:21:00|2019-10-25 22:37:00|                     22.966667|\n",
      "| 2019-10-26|2019-10-26 00:45:00|2019-10-26 22:05:00|                     22.833333|\n",
      "| 2019-10-18|2019-10-18 00:49:00|2019-10-18 21:36:00|                     22.416667|\n",
      "| 2019-10-02|2019-10-02 00:54:00|2019-10-02 20:53:00|                     21.783333|\n",
      "| 2019-10-18|2019-10-18 00:16:00|2019-10-18 21:21:00|                     21.616667|\n",
      "+-----------+-------------------+-------------------+------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the longest trip duration per day using Apache Spark SQL\n",
    "trip_per_day = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        TO_DATE(pickup_datetime) AS pickup_date,\n",
    "        pickup_datetime,\n",
    "        dropOff_datetime,\n",
    "        MAX(\n",
    "            (HOUR(dropOff_datetime) + MINUTE(dropOff_datetime) / 60.0 + SECOND(dropOff_datetime) / 3600.0) -\n",
    "            (HOUR(pickup_datetime) - MINUTE(pickup_datetime) / 60.0 + SECOND(pickup_datetime) / 3600.0)\n",
    "        ) AS longest_trip_duration_in_hours\n",
    "    FROM fhvhv\n",
    "    -- WHERE TO_DATE(pickup_datetime) = TO_DATE(dropOff_datetime) \n",
    "    GROUP BY \n",
    "        pickup_date, pickup_datetime, dropOff_datetime  -- Group by pickup date, pickup time, and drop-off time\n",
    "    ORDER BY longest_trip_duration_in_hours DESC  -- Order by longest trip duration in descending order\n",
    "    LIMIT 10  -- Limit the output to 10 records\n",
    "\"\"\")\n",
    "\n",
    "trip_per_day.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f6d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------------------------+\n",
      "|pickup_date|longest_trip_duration_in_hours_decimal|\n",
      "+-----------+--------------------------------------+\n",
      "| 2019-10-03|                          90829.463043|\n",
      "+-----------+--------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculate the longest trip duration per day using Apache Spark SQL\n",
    "trip_per_day = spark.sql( \"\"\"\n",
    "                SELECT \n",
    "                    TO_DATE(pickup_datetime) AS pickup_date,\n",
    "                    SUM(\n",
    "                        HOUR(dropOff_datetime) + MINUTE(dropOff_datetime) / 60.0 + SECOND(dropOff_datetime) / 3600.0 -\n",
    "                        HOUR(pickup_datetime) + MINUTE(pickup_datetime) / 60.0 + SECOND(pickup_datetime) / 3600.0\n",
    "                    ) AS longest_trip_duration_in_hours_decimal\n",
    "                FROM fhvhv\n",
    "                GROUP BY pickup_date\n",
    "                ORDER BY longest_trip_duration_in_hours_decimal DESC\n",
    "                LIMIT 1;\n",
    "            \"\"\")\n",
    "\n",
    "trip_per_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a768052",
   "metadata": {},
   "source": [
    "Homework Solution 5\n",
    "\n",
    "User Interface\n",
    "\n",
    "- Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "localhost://4040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acf23f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f5a5a3b",
   "metadata": {},
   "source": [
    "Homework Solution 6\n",
    "\n",
    "Least frequent pickup location zone\n",
    "\n",
    "- Load the zone lookup data into a temp view in Spark: Zone Data\n",
    "\n",
    "- Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aadd64e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-06 18:57:08--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\r\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.233.120, 52.217.43.158, 52.216.59.120, ...\r\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.233.120|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 12322 (12K) [application/octet-stream]\r\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\r\n",
      "\r\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2024-03-06 18:57:08 (187 MB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# !wget https://s3.amazonaws.com/nyc-tlc/misc/'taxi+_zone_lookup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a7624eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read\\\n",
    "    .option(\"header\",'true') \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41ed4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d5b59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01_test.ipynb\t\t 10_spark_gcs_schema_local.ipynb  lib\r\n",
      "02_partitioning.ipynb\t fhv_tripdata_2019-10.csv.gz\t  spark-warehouse\r\n",
      "03_sparkDataFrame.ipynb  fhvhv\t\t\t\t  taxi+_zone_lookup.csv\r\n",
      "05_spark_sql.ipynb\t green_taxi\t\t\t  yellow_taxi\r\n",
      "09_connecting_gcs.ipynb  homework.ipynb\t\t\t  zones\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a2e2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the alrady downloaded zone data\n",
    "df_zones = spark.read.parquet('zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "892a24c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4bcbe3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02784', pickup_datetime=datetime.datetime(2019, 10, 1, 9, 55, 38), dropOff_datetime=datetime.datetime(2019, 10, 1, 10, 5, 43), PUlocationID=89, DOlocationID=85, SR_Flag=None, Affiliated_base_number=None),\n",
       " Row(dispatching_base_num='B02429', pickup_datetime=datetime.datetime(2019, 10, 21, 4, 15, 47), dropOff_datetime=datetime.datetime(2019, 10, 21, 4, 36, 4), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B02429'),\n",
       " Row(dispatching_base_num='B01482', pickup_datetime=datetime.datetime(2019, 10, 19, 12, 0), dropOff_datetime=datetime.datetime(2019, 10, 19, 12, 20), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B01482'),\n",
       " Row(dispatching_base_num='B03015', pickup_datetime=datetime.datetime(2019, 10, 11, 14, 28), dropOff_datetime=datetime.datetime(2019, 10, 11, 14, 32, 44), PUlocationID=264, DOlocationID=216, SR_Flag=None, Affiliated_base_number='B03015'),\n",
       " Row(dispatching_base_num='B01529', pickup_datetime=datetime.datetime(2019, 10, 21, 18, 0, 26), dropOff_datetime=datetime.datetime(2019, 10, 21, 18, 7, 21), PUlocationID=264, DOlocationID=80, SR_Flag=None, Affiliated_base_number='B01529')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fhv.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b48d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the main data to the zone data\n",
    "df_join = df_fhv.join(df_zones,\n",
    "                      df_fhv.PUlocationID == df_zones.LocationID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bdbdb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropOff_datetime: timestamp, DOlocationID: int, SR_Flag: string, Affiliated_base_number: string, LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.drop('PUlocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2ff91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(dispatching_base_num='B02784', pickup_datetime=datetime.datetime(2019, 10, 1, 9, 55, 38), dropOff_datetime=datetime.datetime(2019, 10, 1, 10, 5, 43), PUlocationID=89, DOlocationID=85, SR_Flag=None, Affiliated_base_number=None, LocationID='89', Borough='Brooklyn', Zone='Flatbush/Ditmas Park', service_zone='Boro Zone')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad48f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.registerTempTable(\"joined_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49014b14",
   "metadata": {},
   "source": [
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfa32df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_pipkup_lctn = spark.sql(\"\"\"\n",
    "                 SELECT LocationID,\n",
    "                        Zone,\n",
    "                 COUNT(1) AS pickup_count\n",
    "                 FROM\n",
    "                     joined_Table\n",
    "                 GROUP BY\n",
    "                     1,2\n",
    "                ORDER BY\n",
    "                        pickup_count\n",
    "                \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e020f335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:===================================>                  (133 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+\n",
      "|LocationID|                Zone|pickup_count|\n",
      "+----------+--------------------+------------+\n",
      "|         2|         Jamaica Bay|           1|\n",
      "|       105|Governor's Island...|           2|\n",
      "|       111| Green-Wood Cemetery|           5|\n",
      "|        30|       Broad Channel|           8|\n",
      "|       120|     Highbridge Park|          14|\n",
      "|        12|        Battery Park|          15|\n",
      "|       207|Saint Michaels Ce...|          23|\n",
      "|        27|Breezy Point/Fort...|          25|\n",
      "|       154|Marine Park/Floyd...|          26|\n",
      "|         8|        Astoria Park|          29|\n",
      "|       128|    Inwood Hill Park|          39|\n",
      "|       253|       Willets Point|          47|\n",
      "|        96|Forest Park/Highl...|          53|\n",
      "|        34|  Brooklyn Navy Yard|          57|\n",
      "|        59|        Crotona Park|          62|\n",
      "|        58|        Country Club|          77|\n",
      "|        99|     Freshkills Park|          89|\n",
      "|       190|       Prospect Park|          98|\n",
      "|        54|     Columbia Street|         105|\n",
      "|       217|  South Williamsburg|         110|\n",
      "+----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "least_pipkup_lctn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae6527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
